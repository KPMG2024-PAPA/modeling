{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make new tokenizer (with kipris dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/kic/Desktop/jiwon'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1447242/569473941.py:1: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  raw = pd.read_csv('/home/kic/Dataset/corpus.csv', encoding='cp949')\n"
     ]
    }
   ],
   "source": [
    "raw = pd.read_csv('/home/kic/Dataset/corpus.csv', encoding='cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'프라이머와 접촉하기 위해 바닥면에 적용되는 접착제를 더 포함하는 청구항 1에 따른 디퓨저 컵이다.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw['번역문'][152785]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_corpus():\n",
    "    dataset = raw\n",
    "    for start_idx in range(0, len(dataset), 1000):\n",
    "        samples = dataset[start_idx : start_idx + 1000]\n",
    "        yield samples[\"번역문\"]\n",
    "        \n",
    "training_corpus = get_training_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "old_tokenizer = AutoTokenizer.from_pretrained('snunlp/KR-SBERT-V40K-klueNLI-augSTS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이동기계의 3점 히치에 기구를 연결하기 위한 장치는 두 개의 프레임워크, 즉 첫 번째 프레임워크와 두 번째 프레임워크를 포함한다.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['프라이',\n",
       " '##머',\n",
       " '##와',\n",
       " '접촉',\n",
       " '##하기',\n",
       " '위해',\n",
       " '바닥',\n",
       " '##면에',\n",
       " '적용되는',\n",
       " '접',\n",
       " '##착',\n",
       " '##제를',\n",
       " '더',\n",
       " '포함하는',\n",
       " '청구',\n",
       " '##항',\n",
       " '1에',\n",
       " '따른',\n",
       " '디',\n",
       " '##퓨',\n",
       " '##저',\n",
       " '컵',\n",
       " '##이다',\n",
       " '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = raw['번역문'][0]\n",
    "print(example)\n",
    "\n",
    "tokens = old_tokenizer.tokenize('프라이머와 접촉하기 위해 바닥면에 적용되는 접착제를 더 포함하는 청구항 1에 따른 디퓨저 컵이다.')\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['프라이머와',\n",
       " '접촉하기',\n",
       " '위해',\n",
       " '바닥면에',\n",
       " '적용되는',\n",
       " '접착제를',\n",
       " '더',\n",
       " '포함하는',\n",
       " '청구항',\n",
       " '1에',\n",
       " '따른',\n",
       " '디퓨저',\n",
       " '컵',\n",
       " '##이다',\n",
       " '.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize('프라이머와 접촉하기 위해 바닥면에 적용되는 접착제를 더 포함하는 청구항 1에 따른 디퓨저 컵이다.')\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('patent-word-recog-tokenizer/tokenizer_config.json',\n",
       " 'patent-word-recog-tokenizer/special_tokens_map.json',\n",
       " 'patent-word-recog-tokenizer/vocab.txt',\n",
       " 'patent-word-recog-tokenizer/added_tokens.json',\n",
       " 'patent-word-recog-tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"patent-word-recog-tokenizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.2-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting comm>=0.1.3 (from ipywidgets)\n",
      "  Downloading comm-0.2.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /anaconda/envs/v2/lib/python3.8/site-packages (from ipywidgets) (8.12.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /anaconda/envs/v2/lib/python3.8/site-packages (from ipywidgets) (5.7.1)\n",
      "Collecting widgetsnbextension~=4.0.10 (from ipywidgets)\n",
      "  Downloading widgetsnbextension-4.0.10-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab-widgets~=3.0.10 (from ipywidgets)\n",
      "  Downloading jupyterlab_widgets-3.0.10-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: backcall in /anaconda/envs/v2/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: decorator in /anaconda/envs/v2/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /anaconda/envs/v2/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: matplotlib-inline in /anaconda/envs/v2/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in /anaconda/envs/v2/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /anaconda/envs/v2/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /anaconda/envs/v2/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (2.15.1)\n",
      "Requirement already satisfied: stack-data in /anaconda/envs/v2/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: typing-extensions in /anaconda/envs/v2/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /anaconda/envs/v2/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /anaconda/envs/v2/lib/python3.8/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /anaconda/envs/v2/lib/python3.8/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /anaconda/envs/v2/lib/python3.8/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: executing in /anaconda/envs/v2/lib/python3.8/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: asttokens in /anaconda/envs/v2/lib/python3.8/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in /anaconda/envs/v2/lib/python3.8/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six in /anaconda/envs/v2/lib/python3.8/site-packages (from asttokens->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Downloading ipywidgets-8.1.2-py3-none-any.whl (139 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.4/139.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading comm-0.2.1-py3-none-any.whl (7.2 kB)\n",
      "Downloading jupyterlab_widgets-3.0.10-py3-none-any.whl (215 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.0/215.0 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading widgetsnbextension-4.0.10-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: widgetsnbextension, jupyterlab-widgets, comm, ipywidgets\n",
      "  Attempting uninstall: comm\n",
      "    Found existing installation: comm 0.1.2\n",
      "    Uninstalling comm-0.1.2:\n",
      "      Successfully uninstalled comm-0.1.2\n",
      "Successfully installed comm-0.2.1 ipywidgets-8.1.2 jupyterlab-widgets-3.0.10 widgetsnbextension-4.0.10\n"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c9f7f52efed442483a0085feddb1e0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    " \n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Jinja2\n",
      "  Downloading Jinja2-3.1.3-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting MarkupSafe>=2.0 (from Jinja2)\n",
      "  Downloading MarkupSafe-2.1.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Downloading Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.2/133.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading MarkupSafe-2.1.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26 kB)\n",
      "Installing collected packages: MarkupSafe, Jinja2\n",
      "Successfully installed Jinja2-3.1.3 MarkupSafe-2.1.5\n"
     ]
    }
   ],
   "source": [
    "!pip install Jinja2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "226de17a462140c2b768d8aefe1e85db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50ae788cd7774bfab47cd186c1a0c0fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/56.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/zzzzzioni/patent-word-recog-tokenizer/commit/cfde616e743e8a5d613bd05201b78bc3cb74d1a9', commit_message='Upload tokenizer', commit_description='', oid='cfde616e743e8a5d613bd05201b78bc3cb74d1a9', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub(\"patent-word-recog-tokenizer\", use_temp_dir=True, token='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')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add mew tokens to tokenizer (korpat dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_tokenizer = AutoTokenizer.from_pretrained('snunlp/KR-SBERT-V40K-klueNLI-augSTS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 열기\n",
    "with open('add_vocab.txt', 'r', encoding='utf-8') as file:\n",
    "    # 파일 내용 읽기\n",
    "    lines = file.readlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2163713"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokens = set(lines) - set(old_tokenizer.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_tokenizer.add_tokens(list(new_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make new tokenizer (with aihub dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/kic/Desktop/jiwon'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "old_tokenizer = AutoTokenizer.from_pretrained('snunlp/KR-SBERT-V40K-klueNLI-augSTS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1814345/4109457466.py:1: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  raw = pd.read_csv('final.csv', encoding='utf-8')\n"
     ]
    }
   ],
   "source": [
    "raw = pd.read_csv('final.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4489806, 2)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw['text'] = raw['text'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_corpus():\n",
    "    dataset = raw\n",
    "    for start_idx in range(0, len(dataset), 1000):\n",
    "        samples = dataset[start_idx : start_idx + 1000]\n",
    "        yield samples[\"text\"]\n",
    "        \n",
    "training_corpus = get_training_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['프라이',\n",
       " '##머',\n",
       " '##와',\n",
       " '접촉',\n",
       " '##하기',\n",
       " '위해',\n",
       " '바닥',\n",
       " '##면에',\n",
       " '적용되는',\n",
       " '접',\n",
       " '##착',\n",
       " '##제를',\n",
       " '더',\n",
       " '포함하는',\n",
       " '청구',\n",
       " '##항',\n",
       " '1에',\n",
       " '따른',\n",
       " '디',\n",
       " '##퓨',\n",
       " '##저',\n",
       " '컵',\n",
       " '##이다',\n",
       " '.']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokens = old_tokenizer.tokenize('프라이머와 접촉하기 위해 바닥면에 적용되는 접착제를 더 포함하는 청구항 1에 따른 디퓨저 컵이다.')\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['프라이머',\n",
       " '##와',\n",
       " '접촉',\n",
       " '##하기',\n",
       " '위해',\n",
       " '바닥면에',\n",
       " '적용되는',\n",
       " '접착제를',\n",
       " '더',\n",
       " '포함하는',\n",
       " '청구항',\n",
       " '1에',\n",
       " '따른',\n",
       " '디퓨저',\n",
       " '컵',\n",
       " '##이다',\n",
       " '.']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize('프라이머와 접촉하기 위해 바닥면에 적용되는 접착제를 더 포함하는 청구항 1에 따른 디퓨저 컵이다.')\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90117f757bdf4ccda457260e079fbd22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    " \n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('revised30000-patent-word-recog-tokenizer/tokenizer_config.json',\n",
       " 'revised30000-patent-word-recog-tokenizer/special_tokens_map.json',\n",
       " 'revised30000-patent-word-recog-tokenizer/vocab.txt',\n",
       " 'revised30000-patent-word-recog-tokenizer/added_tokens.json',\n",
       " 'revised30000-patent-word-recog-tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"revised30000-patent-word-recog-tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/zzzzzioni/revised30000-patent-word-recog-tokenizer/commit/8277ea10981d434a54fc12c2eeca9552d2b746ab', commit_message='Upload tokenizer', commit_description='', oid='8277ea10981d434a54fc12c2eeca9552d2b746ab', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub(\"revised30000-patent-word-recog-tokenizer\", use_temp_dir=True, token='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')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### embeding check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading config.json: 100%|██████████| 676/676 [00:00<00:00, 737kB/s]\n",
      "Downloading model.safetensors: 100%|██████████| 542M/542M [02:29<00:00, 3.62MB/s] \n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained('zzzzzioni/added-patent-tokens-KR-SBERT-V40K-klueNLI-augSTS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading tokenizer_config.json: 100%|██████████| 4.28M/4.28M [00:01<00:00, 3.06MB/s]\n",
      "Downloading vocab.txt: 100%|██████████| 336k/336k [00:00<00:00, 2.64MB/s]\n",
      "Downloading tokenizer.json: 100%|██████████| 5.49M/5.49M [00:01<00:00, 3.21MB/s]\n",
      "Downloading added_tokens.json: 100%|██████████| 522k/522k [00:00<00:00, 3.77MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 125/125 [00:00<00:00, 907kB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import torch\n",
    "\n",
    "model = AutoModel.from_pretrained('zzzzzioni/added-patent-tokens-KR-SBERT-V40K-klueNLI-augSTS')\n",
    "tokenizer = AutoTokenizer.from_pretrained('zzzzzioni/added-patent-word-recog-tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained('snunlp/KR-SBERT-V40K-klueNLI-augSTS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentences we want sentence embeddings for\n",
    "sen1 = \"본 발명은 차량의 주행환경 또는 노면조건 등에 따라 차량의 좌우 바퀴폭을 조절할 수 있는 좌우 바퀴폭 조절장치에 관한 것이다.\"\n",
    "    # 유사\n",
    "sen2 = \"내측으로 또는 외측으로 슬라이딩 이동가능하도록 상기 제1 축에 축결합되는 제2 축;을 포함하는 것을 특징으로 하는 차량의 좌우 바퀴폭 조절장치에 관한 것이다.\"\n",
    "    # 약간 다름\n",
    "sen3 = \"림 및 스포크 내부에는 폼코어가 삽입되는 구조를 가짐으로써, 경량화를 이루면서 동시에 차량용 휠에서 요구되는 강성을 확보한 차량용 휠을 제공한다.\"\n",
    "    # 다름\n",
    "sen4 = \"본 발명품은 높은 경심에서도 고속으로 안정된 경운작업이 가능한 효과를 제공한다.\"\n",
    "\n",
    "sentences = [sen1, sen2, sen3, sen4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['본', '발명은', '차량의', '주행', '##환경', '또는', '노면', '##조건', '등에', '따라', '차량의', '좌우', '바퀴', '##폭을', '조절할', '수', '있는', '좌우', '바퀴', '##폭', '조절', '##장치에', '관한', '것이다', '.']\n",
      "4 Sentence embeddings:\n",
      "tensor([[-1.1086, -0.8633, -0.1513,  ...,  0.8810, -0.9611,  0.5876],\n",
      "        [-1.2092, -0.6566, -0.4009,  ...,  0.3537, -0.4154,  0.5023],\n",
      "        [-0.5903, -0.6262,  0.3392,  ..., -0.1953, -0.8197, -0.8141],\n",
      "        [-0.4032, -0.3969, -0.3553,  ...,  0.4231, -1.0965,  0.4083]])\n",
      "tensor([0.8701, 0.6919, 0.7264])\n"
     ]
    }
   ],
   "source": [
    "token = tokenizer.tokenize(sen1)\n",
    "print(token)\n",
    "# Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "\n",
    "# Perform pooling. In this case, mean pooling.\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "print(\"4 Sentence embeddings:\")\n",
    "print(sentence_embeddings)\n",
    "\n",
    "cosine_sim = torch.nn.functional.cosine_similarity(sentence_embeddings[0].unsqueeze(0), sentence_embeddings[1:])\n",
    "\n",
    "print(cosine_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19a4da80890d4bf8a52cb74d9c21cfa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.26k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ccaf41e485e4b26b669083490c68c69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/246k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e309514db39f4e66bbcb472ab3b8bed2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/718k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fe607462b59464caa1f6b4976417c82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/695 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "custom_tokenizer = AutoTokenizer.from_pretrained('zzzzzioni/revised30000-patent-word-recog-tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['프라이머',\n",
       " '##와',\n",
       " '접촉',\n",
       " '##하기',\n",
       " '위해',\n",
       " '바닥면에',\n",
       " '적용되는',\n",
       " '접착제를',\n",
       " '더',\n",
       " '포함하는',\n",
       " '청구항',\n",
       " '1에',\n",
       " '따른',\n",
       " '디퓨저',\n",
       " '컵',\n",
       " '##이다',\n",
       " '.']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = custom_tokenizer.tokenize('프라이머와 접촉하기 위해 바닥면에 적용되는 접착제를 더 포함하는 청구항 1에 따른 디퓨저 컵이다.')\n",
    "\n",
    "token"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
